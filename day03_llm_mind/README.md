
# Day 3 â€” Models & LLMs: Tokens, Context, Temperature & All Key Parameters

## ğŸ¯ Goal
Understand what a **model** is, what makes an **LLM** unique, and how key inference parameters (temperature, topâ€‘p, penalties, stop sequences, etc.) shape the modelâ€™s behavior.

---

## ğŸ¤– What is a Model?
A **model** is a trained mathematical function that makes predictions.

Examples:
- Vision model â†’ predicts objects in images  
- Speech model â†’ predicts words  
- Recommendation model â†’ predicts user interests  
- Language model â†’ predicts the next text token  

Models learn patterns from training data and generalize to new situations.

---

## ğŸ§  What is an LLM?
An **LLM (Large Language Model)** is a type of model trained on huge text datasets to predict the next token.

LLMs donâ€™t truly â€œunderstand.â€ They:
- recognize patterns  
- map tokens to probabilities  
- generate the most likely continuation  

---

## ğŸ§© Tokens
Tokens are subâ€‘word pieces like â€œHelâ€, â€œloâ€, â€œworldâ€.  
Token usage controls:
- cost  
- speed  
- context usage  
- memory allocation  

---

## ğŸ§µ Context Window
An LLM can only â€œrememberâ€ a fixed number of tokens.  
If exceeded, earlier tokens get truncated.

---

## ğŸ¨ Temperature
Controls randomness:
- Lower (0â€“0.3) â†’ precise  
- Medium (~0.7) â†’ balanced  
- High (1.0+) â†’ creative  

---

## ğŸ”§ Additional LLM Inference Parameters

### **Topâ€‘p (Nucleus Sampling)**
Choose from tokens whose combined probabilities sum to p.  
Small p â†’ safer, more focused.

### **Topâ€‘k**
Choose only from the top k tokens.  
Smaller k â†’ more deterministic.

### **Max Tokens**
Maximum length of the model's response.

### **Stop Sequences**
Force the model to stop when a pattern appears.

### **Frequency Penalty**
Reduces repetition in output.

### **Presence Penalty**
Encourages introducing new topics.

### **Logit Bias**
Adjust likelihood of specific tokens (force or suppress words).

### **Latency**
Total time it takes for the model to respond, influenced by:
- model size  
- token count  
- network latency  
- output length  

---

## ğŸ§ª Demos Included

### 1) Vanilla TypeScript Demo
Simulates:
- tokenization  
- context truncation  
- temperature sampling  
- simple latency estimation  

Run:
```bash
npm run dev:day3:vanilla
```

### 2) LangChain Framework Demo
Shows:
- temperature 0 vs temperature 1  
- shorter outputs using `maxTokens`  

Run:
```bash
npm run dev:day3:framework
```

---

## ğŸ“š References
- OpenAI Text Generation Parameters â€” https://platform.openai.com/docs/guides/text-generation  
- HuggingFace Generation Tutorial â€” https://huggingface.co/blog/how-to-generate  
- Anthropic API Parameters â€” https://docs.anthropic.com/en/api/messages  
- Topâ€‘k & Topâ€‘p Sampling Research â€” https://arxiv.org/abs/1904.09751  

---