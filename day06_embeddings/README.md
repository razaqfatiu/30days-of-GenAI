# Day 6 â€” Embeddings: How AI Represents Meaning

## ğŸ”„ Connection to Day 5
On Day 5, you learned about inferencing â€” how an LLM â€œthinks,â€ predicts tokens, and responds to prompts.
Today we focus on **understanding**, not generation â€” through embeddings.

Embeddings are the backbone of:
- Search
- RAG
- Recommendations
- Clustering
- Deduplication
- Semantic filtering

---

## ğŸ§  What Are Embeddings?
Embeddings are numeric vectors that represent **meaning**.

Example (simplified):
- â€œI love this productâ€ â†’ [0.8, 0.9, 0.1]
- â€œThis product is greatâ€ â†’ [0.79, 0.88, 0.12]
- â€œThe weather is terrible todayâ€ â†’ [-0.3, 0.1, 0.9]

Similar meaning â†’ vectors close together  
Different meaning â†’ vectors far apart  

Think of embeddings as **GPS coordinates for meaning**.

---

## ğŸ¯ Why Embeddings Matter
Embeddings allow AI systems to:
- Compare meaning of texts
- Perform semantic search
- Group similar items
- Detect anomalies
- Retrieve relevant documents for RAG

Embeddings = **understanding layer**  
LLMs = **generation layer**

---

## ğŸ§® Cosine Similarity (Simple Explanation)
Cosine similarity measures the angle between two vectors.

- 1.0 â†’ extremely similar  
- 0.0 â†’ unrelated  
- -1.0 â†’ opposite  

This is the core operation behind search and retrieval.

---

## ğŸŒ Real-World Use Cases
- Search engines (Google, YouTube, Amazon)
- Spotify & Netflix recommendations
- Fraud and anomaly detection
- Semantic document retrieval
- RAG question answering systems
- Deduplication and clustering

---

## â˜ï¸ OpenAI Embeddings: text-embedding-3-small
We use OpenAIâ€™s text-embedding-3-small because it is:
- cheap
- performant
- widely adopted
- ideal for search & RAG
- beginner-friendly with LangChain

---

## ğŸ“ Best Practices
- Use the same model for doc + query
- Normalize vectors for cosine similarity
- Do NOT embed entire documents (use chunking)
- Store metadata with each embedding
- Cache embeddings instead of recomputing

---

## â— Common Pitfalls
- Chunking too large (hurts retrieval)
- Chunking too small (hurts context)
- Using Euclidean distance instead of cosine similarity
- Mixing embedding providers
- Forgetting metadata

---

## ğŸ§ª Files in This Folder
### code.ts
- Manual cosine similarity
- Fake tiny embeddings
- Ranking by similarity

### framework.ts
- Real embeddings from OpenAI
- LangChain integration
- Mini semantic search demo
- Output ranked results

Run:
```
npm run dev:day6:vanilla
npm run dev:day6:framework
```

---

## ğŸ“š References
- OpenAI Embeddings: https://platform.openai.com/docs/guides/embeddings
- LangChain JS Embeddings: https://js.langchain.com/docs/integrations/text_embedding/openai
- Pinecone Vector Learning: https://www.pinecone.io/learn/vector-embeddings/

---

## â­ Day 7 Preview: Chunking
Before embedding real documents, we must chunk them.
Tomorrow we cover:
- chunk size
- overlap
- sentence preservation
- metadata
- chunking strategies for RAG accuracy
