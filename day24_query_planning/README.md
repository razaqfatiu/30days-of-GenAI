# Day 24 â€” Query Planning & Multi-Hop Retrieval (Deep Dive)

## ðŸ”„ Connection to Day 23
**Day 23** improved *retrieval quality* inside one hop:
- query rewriting
- hybrid retrieval (BM25 + vector)
- reranking
- context assembly

**Day 24** adds a new skill:
> **Plan the retrieval** when one search is not enough.

Because many real questions are **multi-hop**:
- they contain multiple parts
- they require multiple sources
- some parts depend on others

If Day 23 is â€œretrieving betterâ€, Day 24 is â€œretrieving smarterâ€.

---

# The Problem Day 24 Solves
A basic RAG system often does:
> user question â†’ retrieve once â†’ answer

This fails when:
- the question is broad (â€œexplain everythingâ€)
- the question has two goals (â€œcompare X and Y and recommendâ€)
- the answer requires dependencies (â€œfirst define A, then explain B with Aâ€)

**Example (multi-hop)**
> â€œHow does RAG reduce hallucinations, and what role do embeddings play?â€

A single retrieval query might only bring docs about RAG or only about embeddings.
Day 24 teaches how to break it down, route retrieval correctly, and merge evidence.

---

# Day 24 Concepts (In Depth)

## 1ï¸âƒ£ Intent Classification
Intent classification answers:
> â€œWhat type of question is this?â€

Common intents:
- **Fact lookup**: short, direct, usually one hop  
  Example: â€œWhat is an embedding?â€
- **Explanation**: requires concept + mechanism  
  Example: â€œHow does retrieval work in RAG?â€
- **Comparison**: needs two sides + tradeoffs  
  Example: â€œBM25 vs vector searchâ€”when should I use each?â€
- **Procedural**: step-by-step with order  
  Example: â€œHow do I build an ingestion pipeline?â€

### Why intent matters
Intent decides:
- whether multi-hop is needed
- which tools to use (vector vs keyword vs SQL/API)
- whether to run retrieval parallel or sequential
- how to structure the final answer

**Anti-pattern**
Running multi-hop for every question increases:
- latency
- cost
- noise in context

---

## 2ï¸âƒ£ Query Decomposition
Query decomposition turns one complex question into smaller, searchable questions.

**Example**
User question:
> â€œCompare BM25 and vector search, then tell me what to use for invoice IDs.â€

Decomposition:
1. â€œWhat is BM25?â€
2. â€œWhat is vector search?â€
3. â€œWhen is BM25 better?â€
4. â€œWhen is vector search better?â€
5. â€œFor invoice IDs and exact matching, which is better?â€

### Good decomposition rules
- each sub-query should be searchable on its own
- keep sub-queries short and specific
- prefer â€œone concept per sub-queryâ€

### Output of decomposition
A decomposition plan usually includes:
- `query`: the sub-question
- `dependencies`: does it rely on another hop?
- `route`: where it should be retrieved from (vector/keyword/sql/api)
- `priority`: importance for final answer

---

## 3ï¸âƒ£ Retrieval Routing
Routing answers:
> â€œWhere should I search for this sub-query?â€

Typical sources:
- **Vector search**: concepts, explanations, paraphrases  
  Great for â€œmeaningâ€
- **Keyword/BM25**: IDs, exact phrases, names  
  Great for â€œexact matchesâ€
- **SQL**: structured data (users, payments, invoices)  
  Great for â€œfacts in tablesâ€
- **Third-party APIs**: external truth (payments provider, CRM, ticketing)

### Routing examples
- â€œWhat is RAG?â€ â†’ vector
- â€œInvoice INV-9231 statusâ€ â†’ keyword or SQL
- â€œWhatâ€™s the userâ€™s subscription plan?â€ â†’ SQL
- â€œDid Paystack refund payment X?â€ â†’ third-party API

### Why routing matters
If you send an â€œID lookupâ€ query to vector search, it might miss it.
If you send an â€œexplain conceptâ€ query to keyword search, it might retrieve shallow docs.

Routing is a major reason production RAG feels â€œsmartâ€.

---

## 4ï¸âƒ£ Parallel vs Sequential Hops
Once we have sub-queries, we decide execution strategy.

### Parallel hops (faster)
Use parallel execution when sub-queries are independent:
- â€œWhat is RAG?â€
- â€œWhat are embeddings?â€
These can run at the same time to reduce total latency.

### Sequential hops (correctness)
Use sequential execution when one hop depends on another:
- Hop 1: â€œFind the product categoryâ€
- Hop 2: â€œFetch docs for that categoryâ€
- Hop 3: â€œAnswer the userâ€
Here, Hop 2 needs Hop 1â€™s output.

### Practical rule
- Parallel when independent
- Sequential when dependent
- Mix both in complex flows (DAG thinking from Day 19)

---

## 5ï¸âƒ£ Confidence Scoring (Per Hop)
Confidence scoring answers:
> â€œHow reliable is the retrieved evidence for this hop?â€

Signals you can use (simple â†’ advanced):
- **Similarity score** (vector DB score)
- **Keyword match strength** (BM25 score)
- **Reranker score**
- **Heuristics**: empty results, duplicates, too generic
- **LLM judge** (Day 12/20 ideas): â€œIs this evidence relevant?â€

### What to do with confidence
- High confidence â†’ include more context from that hop
- Medium confidence â†’ include but limit tokens
- Low confidence â†’ exclude or ask clarifying question / fallback

**Why this matters**
Without confidence control, one weak hop can pollute the prompt and cause hallucinations.

---

## 6ï¸âƒ£ Context Merging & Budgeting
After multiple hops, you may have too much text.
Context merging decides what the model sees.

### Merging steps
1. **Deduplicate** near-identical chunks
2. **Group** by hop / topic (helps answer structure)
3. **Order** by relevance and dependency (definitions first)
4. **Budget** tokens per hop

### Token budgeting (simple strategy)
Assume you can only send ~N tokens of context.
Split N across hops:
- Hop 1 (core definition): 30%
- Hop 2 (mechanism): 50%
- Hop 3 (examples): 20%

If a hop has low confidence, reduce its budget.

### Why budgeting matters
Even correct retrieval fails if:
- context is too long (truncation)
- the important evidence is buried
- you overwhelm the model with unrelated chunks

---

## 7ï¸âƒ£ Reflection Before Answering
Reflection is a final check:
> â€œDo we have enough evidence to answer all parts?â€

A simple reflection checklist:
- Did we answer each sub-question?
- Do we have strong evidence (confidence) for each part?
- Are there contradictions?
- Is anything missing?

If reflection fails, options:
- retrieve again with refined queries
- ask the user a clarifying question
- answer partially with clear uncertainty

Reflection makes answers more reliable and reduces hallucinations.

---

# How Todayâ€™s Code Implements This
Both `code.ts` and `framework.ts` demonstrate:
- intent classification
- decomposition into hops
- routing (vector vs keyword in demo)
- parallel execution (map/Promise patterns)
- hop confidence scores + filtering
- context merging
- reflection flag before answering

> This is intentionally simplified so the concepts are easy to learn.
> In real production, swap the mock sources with your vector DB, BM25 index, SQL, and APIs (Day 22 tools).

---

## ðŸš€ package.json scripts
```jsonc
"dev:day24:vanilla": "tsx day24_query_planning/code.ts",
"dev:day24:framework": "tsx day24_query_planning/framework.ts"
```

---

## ðŸ“š References
- Pinecone RAG guide: https://www.pinecone.io/learn/retrieval-augmented-generation/
- Hybrid search overview: https://www.elastic.co/what-is/hybrid-search
- Self-Ask + Search (multi-hop idea): https://arxiv.org/abs/2210.03350
- Plan-and-Solve prompting: https://arxiv.org/abs/2305.04091