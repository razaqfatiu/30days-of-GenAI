
# Day 12 ‚Äî Evaluating RAG: Quality, Latency, and Cost

## üîÑ Connection To Previous Days

- **Day 6‚Äì10** gave us: embeddings, chunking, vector DBs, ingestion, and similarity search.
- **Day 11** wired everything into a working **RAG pipeline** (Retrieval + Generation).

Now that we can *answer* questions, the next step as an **AI Engineer** is:

> ‚ÄúIs this system any good ‚Äî and how much does it *cost* to run?‚Äù

Day 12 is about **measuring**, not just building.

---

## üéØ What You‚Äôll Learn Today

We‚Äôll focus on three dimensions:

1. **Quality metrics** (How good are the answers?)
2. **Latency metrics** (How fast is the pipeline?)
3. **Cost / Token metrics** (How expensive is each query?)

You‚Äôll see:

- How to instrument your RAG pipeline with simple timing.
- How to log which chunks were retrieved (for debugging).
- How to capture token usage from the OpenAI API (for cost estimation).
- How frameworks vs. vanilla differ in where you plug in metrics.

---

## üß™ Key Concepts

### 1Ô∏è‚É£ Quality Metrics (Human + Heuristic)

These usually need **human judgment**, but we can assist with structure:

- **Relevance** ‚Äì Does the answer address the question?
- **Groundedness / Faithfulness** ‚Äì Is the answer supported by the retrieved context?
- **Context Utilization** ‚Äì Is the context actually used, or is the model hallucinating?
- **Coverage** ‚Äì Does the answer use all relevant chunks, or just one?

Basic approach:

- Prepare a **small evaluation set**: (question, expected answer or keywords).
- Run your RAG pipeline for each question.
- Manually rate / compare.
- Optionally log everything to a JSON file for later analysis.

In this day‚Äôs code, we show how to log **retrieved chunks and scores** to make manual eval easier.

---

### 2Ô∏è‚É£ Latency Metrics

Latency is **time per query**, broken down by stages:

- Embedding latency (question ‚Üí vector)
- Retrieval latency (search in vector DB / JSON store)
- LLM latency (generation time)
- Total end-to-end latency

We measure these using `Date.now()` in vanilla and timing around calls in the framework version.

You‚Äôll see metrics like:

- `embedMs`
- `retrievalMs`
- `llmMs`
- `totalMs`

In production, you‚Äôd also track:

- p50 / p90 / p99 latency
- timeout rates
- error rates

---

### 3Ô∏è‚É£ Cost / Token Metrics

OpenAI responses often include a `usage` object with fields like:

- `prompt_tokens`
- `completion_tokens`
- `total_tokens`

We can log these to understand:

- How many tokens the context + question consume.
- How many tokens the answer consumes.
- Which queries are most expensive.

You can plug these into your provider‚Äôs pricing to estimate **$ per 1K tokens**.

> ‚ö†Ô∏è Pricing changes over time, so treat any hardcoded values as examples only.
> Always check your provider‚Äôs current pricing page.

In this repo we:

- Log token usage.
- Leave comments where you could compute approximate cost.
- Save metrics into a JSON log file.

---

## üìÇ Files in `day12_evaluation_metrics`

### 1Ô∏è‚É£ `code.ts` ‚Äî Vanilla RAG + Metrics (JSON Store)

This script:

1. Loads `../day09_ingestion_pipeline/day09_local_ingestion_store.json`.
2. Defines a function `runRagWithMetrics(question)` that:
   - Embeds the question using OpenAI.
   - Retrieves top-k chunks using cosine similarity.
   - Calls OpenAI chat completions with question + context.
   - Measures latency for:
     - embedding
     - retrieval
     - generation
   - Captures token usage where available.
   - Produces a `metrics` object containing:
     - question
     - timings `{ embedMs, retrievalMs, llmMs, totalMs }`
     - tokens `{ embeddingTokens, promptTokens, completionTokens, totalTokens }`
     - retrieved chunks (ids + scores)
3. Logs the answer and metrics to the console.
4. Appends metrics to a local JSON log file:
   - `day12_rag_metrics_log.json`

This gives you a **very simple evaluation harness** for observing how your RAG behaves.

---

### 2Ô∏è‚É£ `framework.ts` ‚Äî Metrics Around a LangChain RAG Chain

This script:

1. Re-creates a RAG chain similar to Day 11 using:
   - `ChatOpenAI`
   - `OpenAIEmbeddings`
   - `Chroma.fromExistingCollection(...)`
   - `asRetriever({ k: 3 })`
   - `ChatPromptTemplate`
   - `RunnableSequence`
2. Wraps the chain invocation with timing to capture:
   - `totalMs` ‚Äì time from before `chain.invoke` to after it returns.
3. Logs:
   - the question
   - the final answer
   - retrieved docs‚Äô IDs and strategy
   - the total latency

You could later integrate:

- LangChain callbacks
- LangSmith
- external tracing/observability tools

But Day 12 keeps it **framework-light** and easy to follow.

---

## üöÄ How To Run

In your root `package.json`, add:

```jsonc
"dev:day12:vanilla": "tsx day12_evaluation_metrics/code.ts",
"dev:day12:framework": "tsx day12_evaluation_metrics/framework.ts"
```

Then run:

```bash
npm run dev:day12:vanilla
npm run dev:day12:framework
```

Make sure:

- Day 9 ingestion has been run (so JSON store + Chroma collection exist).
- Your `.env` contains a valid `OPENAI_API_KEY`.

---

## üîë Environment Variables (`.env.sample`)

```bash
OPENAI_API_KEY=your_openai_key_here

# For framework (Chroma)
CHROMA_URL=http://localhost:8000
```

---

## üìä Output Example (Vanilla)

You‚Äôll see logs **similar** to:

```text
Question: Why is chunking important in RAG?

Top retrieved chunks:
 - [chunk:chunk_0] score=0.84 ...
 - [chunk:chunk_5] score=0.81 ...
 - [chunk:chunk_2] score=0.79 ...

Answer:
  (LLM answer...)

Metrics:
{
  "latency": { "embedMs": 120, "retrievalMs": 3, "llmMs": 450, "totalMs": 573 },
  "tokens": { "embeddingTokens": 21, "promptTokens": 150, "completionTokens": 120, "totalTokens": 270 }
}
```

And a `day12_rag_metrics_log.json` file will accumulate run-by-run entries.

---

## üìö References

- RAG Evaluation Concepts (search ‚ÄúRAG evaluation techniques‚Äù)  
- LangChain Evaluation Overview:  
  https://js.langchain.com/docs/guides/evaluation/  
- OpenAI API Token Usage:  
  https://platform.openai.com/docs/guides/usage  
- General Latency & SLO Practices (search ‚Äúlatency SLO p95 p99‚Äù)
